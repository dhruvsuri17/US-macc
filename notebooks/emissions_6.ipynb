{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from ../data/raw/point/2022hc_cb6_22m/inputs/ptegu/egu_cems_2022_POINT_20240615_2022cems_stackfix2_23jul2024_v0.csv...\n",
      "Data loaded. Shape: (126465, 77)\n",
      "Loading CEMS facilities data...\n",
      "Loaded 1343 unique CEMS facility IDs\n",
      "Processed 2054 unique emission locations that match CEMS facility IDs\n",
      "Filtered to 2050 emission points with non-zero emissions\n",
      "Filtered to 2004 power plant emission points using NAICS codes\n",
      "\n",
      "Emissions Summary (in metric tonnes/year):\n",
      "VOC      1.837821e+04\n",
      "NOx      6.414817e+05\n",
      "NH3      1.244030e+04\n",
      "SOx      7.354128e+05\n",
      "PM2_5    9.662813e+04\n",
      "CO2      2.389864e+07\n",
      "dtype: float64\n",
      "\n",
      "Stack Parameter Statistics:\n",
      "            height         diam         temp     velocity\n",
      "count  2004.000000  2004.000000  2004.000000  2004.000000\n",
      "mean     52.914968     4.762724   526.243477    24.359109\n",
      "std      53.650849     2.069812   207.497173    15.119923\n",
      "min       0.000000     0.000000     0.000000     0.000000\n",
      "25%      18.288000     3.352800   366.483333    16.459200\n",
      "50%      38.100000     4.876800   422.038889    21.336000\n",
      "75%      56.235600     5.791200   727.594444    28.214117\n",
      "max     316.382400    16.885920  1140.372222   210.424806\n",
      "\n",
      "Number of unique ORIS facility codes: 1096\n",
      "Top 10 ORIS facilities by number of emission points:\n",
      "oris_facility_code\n",
      "3469     14\n",
      "55690    12\n",
      "55250    12\n",
      "55640    12\n",
      "56948    12\n",
      "3470     11\n",
      "63688    10\n",
      "10       10\n",
      "55279    10\n",
      "7765     10\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Saved complete emissions data (with CO2) to ../data/processed/processed_emissions_with_co2.gpkg\n",
      "Saved InMAP-formatted emissions data to ../data/processed/processed_emissions_for_inmap.gpkg\n",
      "\n",
      "Ready to use with run_sr function!\n",
      "Example: resultsISRM = run_sr(inmap_egu, model='isrm', emis_units='tons/year')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ================================\n",
    "# Define helper functions\n",
    "# ================================\n",
    "\n",
    "# Convert emissions to metric tonnes\n",
    "def convert_to_tonnes(row):\n",
    "    emis_value = row.get('ann_value') if 'ann_value' in row else 0\n",
    "    emis_units = row.get('emissions uom') if 'emissions uom' in row else 'TON'\n",
    "    \n",
    "    if pd.isna(emis_value) or emis_value == '':\n",
    "        return 0\n",
    "        \n",
    "    emis_value = float(emis_value)\n",
    "    \n",
    "    if emis_units == 'LB':\n",
    "        return emis_value * 0.000453592  # Convert pounds to metric tonnes\n",
    "    elif emis_units == 'TON':\n",
    "        return emis_value * 0.90718474  # Convert short tons to metric tonnes\n",
    "    return emis_value  # Already in metric tonnes\n",
    "\n",
    "# Categorize pollutants\n",
    "def categorize_pollutant(row):\n",
    "    pollutant = str(row.get('poll', '')).upper()\n",
    "    \n",
    "    # Attempt to get pollutant description if available\n",
    "    pollutant_desc = ''\n",
    "    if 'pollutant desc' in row:\n",
    "        pollutant_desc = str(row['pollutant desc']).upper()\n",
    "    \n",
    "    if pollutant == 'VOC' or 'VOLATILE ORGANIC' in pollutant_desc:\n",
    "        return 'VOC'\n",
    "    elif pollutant in ['NOX', 'NO', 'NO2'] or ('NITROGEN' in pollutant_desc and 'OXIDE' in pollutant_desc):\n",
    "        return 'NOx'\n",
    "    elif pollutant == 'NH3' or 'AMMONIA' in pollutant_desc:\n",
    "        return 'NH3'\n",
    "    elif pollutant in ['SO2', 'SO4'] or 'SULFUR' in pollutant_desc:\n",
    "        return 'SOx'\n",
    "    elif 'PM25' in pollutant or 'PM2.5' in pollutant_desc or 'PM2_5' in pollutant:\n",
    "        return 'PM2_5'\n",
    "    elif pollutant == 'CO2':\n",
    "        return 'CO2'\n",
    "    return 'Other'\n",
    "\n",
    "# ================================\n",
    "# Process the FF10_POINT file for InMAP compatibility\n",
    "# ================================\n",
    "\n",
    "# Path to your specific FF10_POINT format file\n",
    "file_path = \"../data/raw/point/2022hc_cb6_22m/inputs/ptegu/egu_cems_2022_POINT_20240615_2022cems_stackfix2_23jul2024_v0.csv\"\n",
    "\n",
    "print(f\"Reading data from {file_path}...\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = Path(\"../data/processed\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Count number of header lines to skip\n",
    "with open(file_path, 'r') as f:\n",
    "    header_lines = 0\n",
    "    for line in f:\n",
    "        if line.startswith('#'):\n",
    "            header_lines += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "# Read the FF10_POINT file, skipping header comments\n",
    "df = pd.read_csv(file_path, skiprows=header_lines, low_memory=False)\n",
    "\n",
    "print(f\"Data loaded. Shape: {df.shape}\")\n",
    "\n",
    "# ================================\n",
    "# Load CEMS facilities for filtering\n",
    "# ================================\n",
    "\n",
    "# Load CEMS data to get valid facility IDs\n",
    "print(\"Loading CEMS facilities data...\")\n",
    "emissions_cems = pd.read_csv('../data/2023_annual_emissions_CEMS.csv')\n",
    "cems_plants = set(emissions_cems['Facility ID'].unique())  # Convert to set for faster lookups\n",
    "print(f\"Loaded {len(cems_plants)} unique CEMS facility IDs\")\n",
    "\n",
    "# ================================\n",
    "# Process emissions data\n",
    "# ================================\n",
    "\n",
    "# Initialize dictionaries to store emissions by location\n",
    "location_data = {}\n",
    "\n",
    "# Process each row of the FF10_POINT file\n",
    "for idx, row in df.iterrows():\n",
    "    try:\n",
    "        # Get ORIS facility code\n",
    "        oris_code = row['oris_facility_code'] if 'oris_facility_code' in row else None\n",
    "        \n",
    "        # Clean and validate ORIS code\n",
    "        if oris_code is None or pd.isna(oris_code) or str(oris_code).strip() == '':\n",
    "            continue\n",
    "            \n",
    "        # Try to convert to integer, skipping if not possible\n",
    "        try:\n",
    "            oris_code_int = int(str(oris_code).strip())\n",
    "            if oris_code_int not in cems_plants:\n",
    "                continue\n",
    "        except (ValueError, TypeError):\n",
    "            continue\n",
    "            \n",
    "        # Get coordinates\n",
    "        lon = row['longitude'] if 'longitude' in row else None\n",
    "        lat = row['latitude'] if 'latitude' in row else None\n",
    "        \n",
    "        if lon is None or lat is None or pd.isna(lon) or pd.isna(lat):\n",
    "            continue  # Skip records without valid coordinates\n",
    "        \n",
    "        # Create a location key for aggregating emissions\n",
    "        loc_key = (float(lon), float(lat), str(oris_code_int))  # Include ORIS code in the key\n",
    "        \n",
    "        # Get stack parameters\n",
    "        h = row['stkhgt'] if 'stkhgt' in row else ''\n",
    "        if h != '' and not pd.isna(h):\n",
    "            # Convert feet to meters\n",
    "            h_val = float(h) * 0.3048\n",
    "        else:\n",
    "            h_val = 0\n",
    "            \n",
    "        d = row['stkdiam'] if 'stkdiam' in row else ''\n",
    "        if d != '' and not pd.isna(d):\n",
    "            # Convert feet to meters\n",
    "            d_val = float(d) * 0.3048\n",
    "        else:\n",
    "            d_val = 0\n",
    "            \n",
    "        t = row['stktemp'] if 'stktemp' in row else ''\n",
    "        if t != '' and not pd.isna(t):\n",
    "            # Convert F to K\n",
    "            t_val = (float(t) - 32) * 5.0/9.0 + 273.15\n",
    "        else:\n",
    "            t_val = 0\n",
    "            \n",
    "        v = row['stkvel'] if 'stkvel' in row else ''\n",
    "        if v != '' and not pd.isna(v):\n",
    "            # Convert ft/s to m/s\n",
    "            v_val = float(v) * 0.3048\n",
    "        else:\n",
    "            v_val = 0\n",
    "        \n",
    "        # Get NAICS code\n",
    "        naics = row['naics'] if 'naics' in row else None\n",
    "        naics_str = str(naics) if naics is not None and not pd.isna(naics) else ''\n",
    "        \n",
    "        # If this is the first emission value for this location, initialize\n",
    "        if loc_key not in location_data:\n",
    "            location_data[loc_key] = {\n",
    "                'VOC': 0, 'NOx': 0, 'NH3': 0, 'SOx': 0, 'PM2_5': 0, 'CO2': 0,\n",
    "                'height': h_val, 'diam': d_val, 'temp': t_val, 'velocity': v_val,\n",
    "                'naics': naics_str, 'oris_code': oris_code_int\n",
    "            }\n",
    "        \n",
    "        # Convert emissions to metric tonnes\n",
    "        emissions_tonnes = convert_to_tonnes(row)\n",
    "        \n",
    "        # Get pollutant category\n",
    "        pollutant_category = categorize_pollutant(row)\n",
    "        \n",
    "        # Add emissions to the appropriate category\n",
    "        if pollutant_category in location_data[loc_key]:\n",
    "            location_data[loc_key][pollutant_category] += emissions_tonnes\n",
    "            \n",
    "    except Exception as e:\n",
    "        if idx % 5000 == 0:  # Only print errors every 5000 rows to avoid flooding output\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Convert the aggregated location data into lists for the GeoDataFrame\n",
    "coords = []\n",
    "VOC, NOx, NH3, SOx, PM2_5, CO2 = [], [], [], [], [], []\n",
    "height, diam, temp, velocity = [], [], [], []\n",
    "naics_codes = []\n",
    "oris_codes = []\n",
    "\n",
    "for loc, data in location_data.items():\n",
    "    coords.append(Point(loc[0], loc[1]))\n",
    "    VOC.append(data['VOC'])\n",
    "    NOx.append(data['NOx'])\n",
    "    NH3.append(data['NH3'])\n",
    "    SOx.append(data['SOx'])\n",
    "    PM2_5.append(data['PM2_5'])\n",
    "    CO2.append(data['CO2'])\n",
    "    height.append(data['height'])\n",
    "    diam.append(data['diam'])\n",
    "    temp.append(data['temp'])\n",
    "    velocity.append(data['velocity'])\n",
    "    naics_codes.append(data['naics'])\n",
    "    oris_codes.append(data['oris_code'])\n",
    "\n",
    "print(f\"Processed {len(coords)} unique emission locations that match CEMS facility IDs\")\n",
    "\n",
    "# Create the emissions GeoDataFrame\n",
    "data_dict = {\n",
    "    \"VOC\": VOC, \n",
    "    \"NOx\": NOx, \n",
    "    \"NH3\": NH3, \n",
    "    \"SOx\": SOx, \n",
    "    \"PM2_5\": PM2_5,\n",
    "    \"CO2\": CO2,\n",
    "    \"height\": height, \n",
    "    \"diam\": diam, \n",
    "    \"temp\": temp, \n",
    "    \"velocity\": velocity,\n",
    "    \"naics_code\": naics_codes,\n",
    "    \"oris_facility_code\": oris_codes\n",
    "}\n",
    "\n",
    "emis = gpd.GeoDataFrame(data_dict, geometry=coords, crs='epsg:4269')\n",
    "\n",
    "# Filter out any rows with all zeros for InMAP-relevant emissions\n",
    "emis = emis[(emis['VOC'] > 0) | (emis['NOx'] > 0) | (emis['NH3'] > 0) | \n",
    "            (emis['SOx'] > 0) | (emis['PM2_5'] > 0)]\n",
    "\n",
    "print(f\"Filtered to {len(emis)} emission points with non-zero emissions\")\n",
    "\n",
    "# ================================\n",
    "# Filter for power plants\n",
    "# ================================\n",
    "\n",
    "# This is already EGU CEMS data, so all should be power plants\n",
    "# But check NAICS codes if available for validation\n",
    "egu_naics_prefixes = ['2211', '221111', '221112', '221113', '221114', '221115', \n",
    "                      '221116', '221117', '221118', '221121', '221122']\n",
    "\n",
    "# Create a mask for power plants\n",
    "is_power_plant = emis['naics_code'].apply(\n",
    "    lambda x: any(str(x).startswith(prefix) for prefix in egu_naics_prefixes) \n",
    "              if x else False\n",
    ")\n",
    "\n",
    "# Apply the mask to filter for power plants if NAICS codes are available and identified\n",
    "if is_power_plant.any():\n",
    "    egu_emis = emis[is_power_plant].copy()\n",
    "    print(f\"Filtered to {len(egu_emis)} power plant emission points using NAICS codes\")\n",
    "else:\n",
    "    # Since this is EGU CEMS data, we can use all points even without NAICS codes\n",
    "    print(\"Using all emission points (dataset is already for power plants - EGU CEMS)\")\n",
    "    egu_emis = emis.copy()\n",
    "\n",
    "# ================================\n",
    "# Inspect and validate the emissions data\n",
    "# ================================\n",
    "\n",
    "# Display summary statistics - properly handle the geometry column\n",
    "print(\"\\nEmissions Summary (in metric tonnes/year):\")\n",
    "# Convert to DataFrame to exclude geometry column for sum operation\n",
    "emissions_df = pd.DataFrame(egu_emis.drop(columns=['geometry']))\n",
    "emission_sums = emissions_df[[\"VOC\", \"NOx\", \"NH3\", \"SOx\", \"PM2_5\", \"CO2\"]].sum()\n",
    "print(emission_sums)\n",
    "\n",
    "print(\"\\nStack Parameter Statistics:\")\n",
    "stack_stats = emissions_df[[\"height\", \"diam\", \"temp\", \"velocity\"]].describe()\n",
    "print(stack_stats)\n",
    "\n",
    "# Print the distribution of ORIS facility codes\n",
    "print(\"\\nNumber of unique ORIS facility codes:\", egu_emis['oris_facility_code'].nunique())\n",
    "print(\"Top 10 ORIS facilities by number of emission points:\")\n",
    "print(egu_emis['oris_facility_code'].value_counts().head(10))\n",
    "\n",
    "# ================================\n",
    "# Prepare Final Output for InMAP\n",
    "# ================================\n",
    "\n",
    "# Create a copy with just the InMAP required columns\n",
    "inmap_columns = [\"VOC\", \"NOx\", \"NH3\", \"SOx\", \"PM2_5\", \"height\", \"diam\", \"temp\", \"velocity\", \"geometry\"]\n",
    "inmap_egu = egu_emis[inmap_columns].copy()\n",
    "\n",
    "# Save both versions - one with CO2 for climate modeling, one for InMAP\n",
    "full_output_file = f\"{output_dir}/processed_emissions_with_co2.gpkg\"\n",
    "egu_emis.to_file(full_output_file, driver=\"GPKG\")\n",
    "print(f\"\\nSaved complete emissions data (with CO2) to {full_output_file}\")\n",
    "\n",
    "inmap_output_file = f\"{output_dir}/processed_emissions_for_inmap.gpkg\"\n",
    "inmap_egu.to_file(inmap_output_file, driver=\"GPKG\")\n",
    "print(f\"Saved InMAP-formatted emissions data to {inmap_output_file}\")\n",
    "\n",
    "print(\"\\nReady to use with run_sr function!\")\n",
    "print(\"Example: resultsISRM = run_sr(inmap_egu, model='isrm', emis_units='tons/year')\")\n",
    "\n",
    "# The emissions GeoDataFrame is now in the exact format needed for InMAP\n",
    "egu_gdf = inmap_egu  # For use with run_sr\n",
    "egu_with_co2 = egu_emis  # For use with climate damage calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below are the helper functions for the run_sr function\n",
    "import time\n",
    "import numpy as np\n",
    "import zarr\n",
    "from shapely.geometry import Polygon, Point\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import s3fs\n",
    "\n",
    "def rect(i, w, s, e, n):\n",
    "    x = [w[i], e[i], e[i], w[i], w[i]]\n",
    "    y = [s[i], s[i], n[i], n[i], s[i]]\n",
    "    return x, y\n",
    "\n",
    "def poly(sr):\n",
    "    ret = []\n",
    "    w = sr[\"W\"][:]\n",
    "    s = sr[\"S\"][:]\n",
    "    e = sr[\"E\"][:]\n",
    "    n = sr[\"N\"][:]\n",
    "    for i in range(52411):\n",
    "        x, y = rect(i, w, s, e, n)\n",
    "        ret.append(Polygon([[x[0],y[0]],[x[1],y[1]],[x[2],y[2]],\n",
    "                            [x[3],y[3]],[x[4],y[4]]]))\n",
    "    return ret\n",
    "\n",
    "# Define the run_sr function\n",
    "def run_sr(emis, model, emis_units=\"tons/year\"):\n",
    "    start = time.time()\n",
    "    \n",
    "    # Load spatial receptor grid (SR)\n",
    "    url = 's3://inmap-model/isrm_v1.2.1.zarr/'\n",
    "    fs = s3fs.S3FileSystem(anon=True, client_kwargs={\"region_name\": \"us-east-2\"})\n",
    "    sr = zarr.open(\n",
    "        store=url,\n",
    "        mode=\"r\",\n",
    "        storage_options={\"anon\": True, \"client_kwargs\": {\"region_name\": \"us-east-2\"}}\n",
    "    )   \n",
    "\n",
    "    # Build the grid geometry\n",
    "    p = poly(sr)\n",
    "    print(\"Making polygons as geometry.\")\n",
    "\n",
    "    # Create grid GeoDataFrame\n",
    "    df = pd.DataFrame({'Location': range(52411)})\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=p, crs=\"+proj=lcc +lat_1=33.000000 +lat_2=45.000000 +lat_0=40.000000 +lon_0=-97.000000 +x_0=0 +y_0=0 +a=6370997.000000 +b=6370997.000000 +to_meter=1\")\n",
    "    \n",
    "    # Ensure emis has CRS set correctly\n",
    "    if emis.crs is None:\n",
    "        print(\"Warning: emis CRS is None. Assigning default CRS (WGS84).\")\n",
    "        emis = emis.set_crs(\"EPSG:4326\")\n",
    "\n",
    "    # Convert emissions to match grid CRS\n",
    "    emis = emis.to_crs(gdf.crs)\n",
    "\n",
    "    # Spatial join (match emissions to grid)\n",
    "    join_right_df = gdf.sjoin(emis, how=\"right\")\n",
    "\n",
    "    # Debugging: Print missing locations\n",
    "    missing_count = join_right_df.Location.isna().sum()\n",
    "    print(f\"Finished joining dataframes. Missing locations: {missing_count}\")\n",
    "\n",
    "    # Drop NaN locations if any exist\n",
    "    join_right_df = join_right_df.dropna(subset=[\"Location\"])\n",
    "    \n",
    "    index = join_right_df.Location.astype(int).tolist()  # Ensure integer type\n",
    "\n",
    "    # Get unique indices for emissions\n",
    "    ppl = np.unique(index).tolist()\n",
    "\n",
    "    # Create dictionary for mapping locations to index\n",
    "    dictionary = {ppl[i]: i for i in range(len(ppl))}\n",
    "\n",
    "    # Load Source-Receptor (SR) matrix data\n",
    "    SOA = sr['SOA'].get_orthogonal_selection(([0], ppl, slice(None)))\n",
    "    print(\"SOA data allocated.\")\n",
    "    pNO3 = sr['pNO3'].get_orthogonal_selection(([0], ppl, slice(None)))\n",
    "    print(\"pNO3 data allocated.\")\n",
    "    pNH4 = sr['pNH4'].get_orthogonal_selection(([0], ppl, slice(None)))\n",
    "    print(\"pNH4 data allocated.\")\n",
    "    pSO4 = sr['pSO4'].get_orthogonal_selection(([0], ppl, slice(None)))\n",
    "    print(\"pSO4 data allocated.\")\n",
    "    PM25 = sr['PrimaryPM25'].get_orthogonal_selection(([0], ppl, slice(None)))\n",
    "    print(\"PrimaryPM25 data allocated.\")\n",
    "\n",
    "    # Initialize output data arrays\n",
    "    SOA_data, pNO3_data, pNH4_data, pSO4_data, PM25_data = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    # Calculate pollution data using emissions\n",
    "    for i in range(len(index)):\n",
    "        loc_idx = dictionary[index[i]]  # Get correct index\n",
    "        SOA_data += SOA[0, loc_idx, :] * emis.VOC.iloc[i]\n",
    "        pNO3_data += pNO3[0, loc_idx, :] * emis.NOx.iloc[i]\n",
    "        pNH4_data += pNH4[0, loc_idx, :] * emis.NH3.iloc[i]\n",
    "        pSO4_data += pSO4[0, loc_idx, :] * emis.SOx.iloc[i]\n",
    "        PM25_data += PM25[0, loc_idx, :] * emis.PM2_5.iloc[i]\n",
    "\n",
    "    data = SOA_data + pNO3_data + pNH4_data + pSO4_data + PM25_data\n",
    "\n",
    "    print(\"Accessing data.\")\n",
    "\n",
    "    # Apply emission unit conversion factor\n",
    "    fact = 28766.639 if emis_units == \"tons/year\" else 1\n",
    "\n",
    "    # Compute final pollution metrics\n",
    "    TotalPM25 = fact * data\n",
    "    TotalPop = sr['TotalPop'][0:52411]\n",
    "    MortalityRate = sr['MortalityRate'][0:52411]\n",
    "    deathsK = (np.exp(np.log(1.06)/10 * TotalPM25) - 1) * TotalPop * 1.04658 * MortalityRate / 100000 * 1.02523\n",
    "    deathsL = (np.exp(np.log(1.14)/10 * TotalPM25) - 1) * TotalPop * 1.04658 * MortalityRate / 100000 * 1.02523\n",
    "\n",
    "    # Create output GeoDataFrame\n",
    "    ret = gpd.GeoDataFrame(pd.DataFrame({\n",
    "        'SOA': fact * SOA_data,\n",
    "        'pNO3': fact * pNO3_data,\n",
    "        'pNH4': fact * pNH4_data,\n",
    "        'pSO4': fact * pSO4_data,\n",
    "        'PrimaryPM25': fact * PM25_data,\n",
    "        'TotalPM25': TotalPM25,\n",
    "        'deathsK': deathsK,\n",
    "        'deathsL': deathsL\n",
    "    }), geometry=p[:52411])\n",
    "\n",
    "    print(f\"Finished ({time.time() - start:.0f} seconds)\")\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import pandas as pd\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import matplotlib.gridspec as gridspec\n",
    "from pathlib import Path\n",
    "\n",
    "# ================================\n",
    "# Step 1: Load the necessary data\n",
    "# ================================\n",
    "\n",
    "# Load the emissions data with CO2 values\n",
    "egu_with_co2 = gpd.read_file(\"../data/processed/processed_emissions_with_co2.gpkg\")\n",
    "\n",
    "# Load the InMAP-formatted emissions data for the SR model\n",
    "inmap_egu = gpd.read_file(\"../data/processed/processed_emissions_for_inmap.gpkg\")\n",
    "\n",
    "# Run the Source-Receptor model with the InMAP-formatted data\n",
    "print(\"Running Source-Receptor model...\")\n",
    "resultsISRM = run_sr(inmap_egu, model=\"isrm\", emis_units=\"tons/year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Step 2: Prepare county-level data for visualization\n",
    "# ================================\n",
    "\n",
    "# Load county boundaries\n",
    "print(\"Loading county boundaries...\")\n",
    "us_counties = gpd.read_file(\"../data/raw/cb_2018_us_county_500k/cb_2018_us_county_500k.shp\")\n",
    "\n",
    "# Convert counties to match emissions data CRS\n",
    "us_counties = us_counties.to_crs(resultsISRM.crs)\n",
    "\n",
    "# Perform spatial join to assign each grid cell to a county\n",
    "print(\"Joining health impacts to counties...\")\n",
    "results_county = resultsISRM.sjoin(us_counties, how=\"left\", predicate=\"intersects\")\n",
    "\n",
    "# Aggregate health impacts by county\n",
    "county_summary = results_county.groupby(\"NAME\").agg({\n",
    "    \"SOA\": \"sum\",\n",
    "    \"pNO3\": \"sum\",\n",
    "    \"pNH4\": \"sum\",\n",
    "    \"pSO4\": \"sum\",\n",
    "    \"PrimaryPM25\": \"sum\",\n",
    "    \"TotalPM25\": \"sum\",\n",
    "    \"deathsK\": \"sum\",\n",
    "    \"deathsL\": \"sum\"\n",
    "}).reset_index()\n",
    "\n",
    "# Merge summary with county shapefile for visualization\n",
    "us_counties = us_counties.merge(county_summary, on=\"NAME\", how=\"left\")\n",
    "\n",
    "# Exclude Alaska, Hawaii, and Puerto Rico\n",
    "us_counties = us_counties[~us_counties['STATEFP'].isin([\"02\", \"15\", \"72\"])]\n",
    "\n",
    "# ================================\n",
    "# Step 3: Calculate health damages\n",
    "# ================================\n",
    "\n",
    "# Value of a Statistical Life in dollars\n",
    "VSL = 13.2e6  # $13.2 million per life\n",
    "\n",
    "# Calculate health damages for each county\n",
    "us_counties['HealthDamages'] = us_counties['deathsK'] * VSL\n",
    "\n",
    "# Handle NaN values\n",
    "us_counties.loc[:, 'HealthDamages'] = us_counties['HealthDamages'].fillna(0)\n",
    "\n",
    "# ================================\n",
    "# Step 4: Calculate climate damages using actual CO2 emissions\n",
    "# ================================\n",
    "\n",
    "# Join the CO2 emissions to counties for climate damage calculation\n",
    "print(\"Calculating climate damages from CO2 emissions...\")\n",
    "\n",
    "# Spatial join to assign CO2 emissions to counties\n",
    "egu_with_co2 = egu_with_co2.to_crs(us_counties.crs)\n",
    "co2_county = egu_with_co2.sjoin(us_counties[['GEOID', 'geometry']], how=\"inner\", predicate=\"within\")\n",
    "\n",
    "# Aggregate CO2 emissions by county\n",
    "co2_summary = co2_county.groupby('GEOID')['CO2'].sum().reset_index()\n",
    "\n",
    "# Social Cost of Carbon (SCC) in dollars per ton\n",
    "SCC = 51  # $51 per ton of CO2 (based on EPA estimates)\n",
    "\n",
    "# Calculate climate damages for each county\n",
    "co2_summary['ClimateDamages'] = co2_summary['CO2'] * SCC\n",
    "\n",
    "# Merge climate damages with county data\n",
    "us_counties = us_counties.merge(co2_summary[['GEOID', 'ClimateDamages']], on='GEOID', how='left')\n",
    "us_counties['ClimateDamages'] = us_counties['ClimateDamages'].fillna(0)\n",
    "\n",
    "# Calculate total damages (health + climate)\n",
    "us_counties['TotalDamages'] = us_counties['HealthDamages'] + us_counties['ClimateDamages']\n",
    "\n",
    "# ================================\n",
    "# Step 5: Create dual-panel visualization\n",
    "# ================================\n",
    "\n",
    "# Define bins and colors for damages\n",
    "bins = [0, 1e6, 5e6, 10e6, 50e6, 100e6, 500e6, 1e9, 5e9, float(\"inf\")]\n",
    "colors = ['#ffedea', '#ffcec5', '#ffad9f', '#ff7f66', '#ff4d33', \n",
    "          '#ff1a00', '#cc1600', '#990f00', '#660a00']\n",
    "\n",
    "# Create a discrete colormap\n",
    "cmap = mcolors.ListedColormap(colors)\n",
    "\n",
    "# Bin the health and total damages data\n",
    "us_counties['HealthDamages_Binned'] = pd.cut(us_counties['HealthDamages'], bins=bins, labels=False, include_lowest=True)\n",
    "us_counties['TotalDamages_Binned'] = pd.cut(us_counties['TotalDamages'], bins=bins, labels=False, include_lowest=True)\n",
    "\n",
    "# Create figure with two panels\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "gs = gridspec.GridSpec(1, 2, width_ratios=[1, 1])\n",
    "\n",
    "# Left panel - Health Damages\n",
    "ax1 = plt.subplot(gs[0])\n",
    "us_counties.plot(column='HealthDamages_Binned', cmap=cmap, linewidth=0.3, edgecolor=\"black\", \n",
    "                 ax=ax1, legend=False)\n",
    "ax1.set_title('Health Damages from Power Plant Emissions', fontsize=16)\n",
    "ax1.axis('off')\n",
    "ax1.set_aspect(1.3)\n",
    "ax1.set_xlim(-130, -60)\n",
    "ax1.set_ylim(20, 55)\n",
    "\n",
    "# Calculate total damages across all counties\n",
    "total_health = us_counties['HealthDamages'].sum() / 1e9  # billions of dollars\n",
    "total_climate = us_counties['ClimateDamages'].sum() / 1e9  # billions of dollars\n",
    "total_combined = us_counties['TotalDamages'].sum() / 1e9  # billions of dollars\n",
    "\n",
    "# Add text annotation with total health damages\n",
    "ax1.text(0.05, 0.05, f'Total Health Damages: ${total_health:.2f} billion', \n",
    "         transform=ax1.transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "# Right panel - Total Damages (Health + Climate)\n",
    "ax2 = plt.subplot(gs[1])\n",
    "us_counties.plot(column='TotalDamages_Binned', cmap=cmap, linewidth=0.3, edgecolor=\"black\", \n",
    "                 ax=ax2, legend=False)\n",
    "ax2.set_title('Total Damages (Health + Climate) from Power Plant Emissions', fontsize=16)\n",
    "ax2.axis('off')\n",
    "ax2.set_aspect(1.3)\n",
    "ax2.set_xlim(-130, -60)\n",
    "ax2.set_ylim(20, 55)\n",
    "\n",
    "# Add text annotation with total combined damages\n",
    "ax2.text(0.05, 0.05, \n",
    "         f'Total Combined Damages: ${total_combined:.2f} billion\\n' +\n",
    "         f'Climate Damages: ${total_climate:.2f} billion\\n' + \n",
    "         f'Using actual CO2 emissions data', \n",
    "         transform=ax2.transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "# Create formatted legend labels\n",
    "legend_labels = [\n",
    "    '$0 - $1M', \n",
    "    '$1M - $5M', \n",
    "    '$5M - $10M', \n",
    "    '$10M - $50M', \n",
    "    '$50M - $100M', \n",
    "    '$100M - $500M', \n",
    "    '$500M - $1B', \n",
    "    '$1B - $5B', \n",
    "    '$5B+'\n",
    "]\n",
    "\n",
    "# Create custom legend patches\n",
    "legend_patches = [\n",
    "    mpatches.Patch(color=colors[i], label=legend_labels[i]) \n",
    "    for i in range(len(legend_labels))\n",
    "]\n",
    "\n",
    "# Add the custom legend to the figure\n",
    "fig.legend(handles=legend_patches, title=\"Damages ($)\", \n",
    "           loc=\"lower center\", ncol=len(legend_labels), bbox_to_anchor=(0.5, 0.02))\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout(rect=[0, 0.07, 1, 0.98])\n",
    "plt.suptitle('Comparison of Health vs. Combined (Health + Climate) Damages from Power Plant Emissions', fontsize=18, y=0.98)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = Path(\"../figures\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(\"../figures/health_climate_damages_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Show the figure\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualization complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
