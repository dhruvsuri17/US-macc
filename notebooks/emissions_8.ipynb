{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting emissions data processing...\n",
      "Loading CEMS facilities data...\n",
      "Loaded 1343 unique CEMS facility IDs\n",
      "Reading EGU CEMS data from ../data/raw/point/2022hc_cb6_22m/inputs/ptegu/egu_cems_2022_POINT_20240615_2022cems_stackfix2_23jul2024_v0.csv...\n",
      "EGU data loaded. Shape: (126465, 77)\n",
      "Found 1106 unique facility IDs matching CEMS plants\n",
      "Reading NEI data from ../data/raw/2021_NEI_Facility_summary.csv...\n",
      "NEI file read with comma delimiter\n",
      "NEI data loaded. Shape: (2005169, 33)\n",
      "Filtered NEI data to 46417 rows matching facility IDs\n",
      "Processing emissions data...\n",
      "Created GeoDataFrame with 1097 facilities\n",
      "Filtered to 1069 power plant facilities\n",
      "Saved all facilities data to ../data/processed/processed_all_facilities_emissions.gpkg\n",
      "Saved processed power plant data to ../data/processed/processed_egu_emissions.gpkg\n",
      "Saved InMAP-formatted emissions data to ../data/processed/processed_emissions_for_inmap.gpkg\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Starting emissions data processing...\")\n",
    "\n",
    "# ================================\n",
    "# Step 1: Load CEMS data for facility filtering\n",
    "# ================================\n",
    "\n",
    "print(\"Loading CEMS facilities data...\")\n",
    "emissions_cems = pd.read_csv('../data/2023_annual_emissions_CEMS.csv')\n",
    "cems_plants = set(emissions_cems['Facility ID'].unique())  # Convert to set for faster lookups\n",
    "print(f\"Loaded {len(cems_plants)} unique CEMS facility IDs\")\n",
    "\n",
    "# ================================\n",
    "# Step 2: Load and filter FF10_POINT data based on CEMS facilities\n",
    "# ================================\n",
    "\n",
    "# Path to your specific FF10_POINT format file\n",
    "egu_file_path = \"../data/raw/point/2022hc_cb6_22m/inputs/ptegu/egu_cems_2022_POINT_20240615_2022cems_stackfix2_23jul2024_v0.csv\"\n",
    "\n",
    "print(f\"Reading EGU CEMS data from {egu_file_path}...\")\n",
    "\n",
    "# Count number of header lines to skip\n",
    "with open(egu_file_path, 'r') as f:\n",
    "    header_lines = 0\n",
    "    for line in f:\n",
    "        if line.startswith('#'):\n",
    "            header_lines += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "# Read the FF10_POINT file, skipping header comments\n",
    "egu_df = pd.read_csv(egu_file_path, skiprows=header_lines, low_memory=False)\n",
    "print(f\"EGU data loaded. Shape: {egu_df.shape}\")\n",
    "\n",
    "# Filter to keep only rows where oris_facility_code is in CEMS plants\n",
    "filtered_egu = []\n",
    "matched_facility_ids = set()  # Track unique facility IDs that match CEMS\n",
    "\n",
    "# Create a dictionary to store stack parameters by facility ID\n",
    "stack_params_by_facility = {}\n",
    "\n",
    "for idx, row in egu_df.iterrows():\n",
    "    try:\n",
    "        # Get ORIS facility code\n",
    "        oris_code = row.get('oris_facility_code', None)\n",
    "        facility_id = row.get('facility_id', None)\n",
    "        \n",
    "        # Skip if either is missing\n",
    "        if oris_code is None or pd.isna(oris_code) or facility_id is None or pd.isna(facility_id):\n",
    "            continue\n",
    "            \n",
    "        # Try to convert ORIS code to integer\n",
    "        try:\n",
    "            oris_code_int = int(str(oris_code).strip())\n",
    "            # Check if in CEMS plants\n",
    "            if oris_code_int in cems_plants:\n",
    "                filtered_egu.append(row)\n",
    "                facility_id_str = str(facility_id).strip()\n",
    "                matched_facility_ids.add(facility_id_str)\n",
    "                \n",
    "                # Store stack parameters for this facility\n",
    "                if facility_id_str not in stack_params_by_facility:\n",
    "                    stack_params_by_facility[facility_id_str] = {\n",
    "                        'orispl': oris_code_int,\n",
    "                        'stkhgt': row.get('stkhgt', np.nan),\n",
    "                        'stkdiam': row.get('stkdiam', np.nan),\n",
    "                        'stktemp': row.get('stktemp', np.nan),\n",
    "                        'stkvel': row.get('stkvel', np.nan),\n",
    "                        # Get the counts for weighted averaging\n",
    "                        'count': 1\n",
    "                    }\n",
    "                else:\n",
    "                    # Update with new values (we'll average later)\n",
    "                    current = stack_params_by_facility[facility_id_str]\n",
    "                    current['stkhgt'] = np.nansum([current['stkhgt'], row.get('stkhgt', np.nan)])\n",
    "                    current['stkdiam'] = np.nansum([current['stkdiam'], row.get('stkdiam', np.nan)])\n",
    "                    current['stktemp'] = np.nansum([current['stktemp'], row.get('stktemp', np.nan)])\n",
    "                    current['stkvel'] = np.nansum([current['stkvel'], row.get('stkvel', np.nan)])\n",
    "                    current['count'] += 1\n",
    "        except (ValueError, TypeError):\n",
    "            continue\n",
    "    except Exception as e:\n",
    "        if idx % 5000 == 0:  # Limit error output\n",
    "            print(f\"Error processing EGU row {idx}: {e}\")\n",
    "\n",
    "print(f\"Found {len(matched_facility_ids)} unique facility IDs matching CEMS plants\")\n",
    "\n",
    "# Calculate average stack parameters by facility\n",
    "for facility_id, params in stack_params_by_facility.items():\n",
    "    count = params['count']\n",
    "    if count > 0:\n",
    "        params['stkhgt'] = params['stkhgt'] / count\n",
    "        params['stkdiam'] = params['stkdiam'] / count\n",
    "        params['stktemp'] = params['stktemp'] / count\n",
    "        params['stkvel'] = params['stkvel'] / count\n",
    "\n",
    "# ================================\n",
    "# Step 3: Load and filter NEI data based on matched facility IDs\n",
    "# ================================\n",
    "\n",
    "# Path to your NEI facility summary CSV file\n",
    "nei_file_path = \"../data/raw/2021_NEI_Facility_summary.csv\"\n",
    "print(f\"Reading NEI data from {nei_file_path}...\")\n",
    "\n",
    "# Read the NEI CSV file\n",
    "try:\n",
    "    nei_df = pd.read_csv(nei_file_path, sep=',', low_memory=False)\n",
    "    print(\"NEI file read with comma delimiter\")\n",
    "except:\n",
    "    try:\n",
    "        nei_df = pd.read_csv(nei_file_path, low_memory=False)\n",
    "        print(\"NEI file read with default delimiter\")\n",
    "    except:\n",
    "        nei_df = pd.read_csv(nei_file_path, sep=None, engine='python', low_memory=False)\n",
    "        print(\"NEI file read with automatic delimiter detection\")\n",
    "\n",
    "print(f\"NEI data loaded. Shape: {nei_df.shape}\")\n",
    "\n",
    "# Filter NEI data to keep only rows with eis facility id matching matched_facility_ids\n",
    "filtered_nei = []\n",
    "for idx, row in nei_df.iterrows():\n",
    "    try:\n",
    "        eis_id = row.get('eis facility id', None)\n",
    "        if eis_id is not None and not pd.isna(eis_id) and str(eis_id).strip() in matched_facility_ids:\n",
    "            filtered_nei.append(row)\n",
    "    except Exception as e:\n",
    "        if idx % 10000 == 0:  # Limit error output\n",
    "            print(f\"Error processing NEI row {idx}: {e}\")\n",
    "\n",
    "print(f\"Filtered NEI data to {len(filtered_nei)} rows matching facility IDs\")\n",
    "\n",
    "# Convert filtered_nei list to DataFrame\n",
    "filtered_nei_df = pd.DataFrame(filtered_nei)\n",
    "\n",
    "# ================================\n",
    "# Step 4: Process filtered NEI data to create GeoDataFrame for InMAP\n",
    "# ================================\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = Path(\"../data/processed\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Convert emissions to metric tonnes\n",
    "def convert_to_tonnes(row):\n",
    "    if row['emissions uom'] == 'LB':\n",
    "        return float(row['total emissions']) * 0.000453592  # Convert pounds to metric tonnes\n",
    "    elif row['emissions uom'] == 'TON':\n",
    "        return float(row['total emissions']) * 0.90718474  # Convert short tons to metric tonnes\n",
    "    return float(row['total emissions'])  # Already in metric tonnes\n",
    "\n",
    "# Categorize pollutants\n",
    "def categorize_pollutant(row):\n",
    "    pollutant = str(row['pollutant code']).upper()\n",
    "    pollutant_desc = str(row['pollutant desc']).upper() if 'pollutant desc' in row else \"\"\n",
    "\n",
    "    if pollutant == 'VOC' or 'VOLATILE ORGANIC' in pollutant_desc:\n",
    "        return 'VOC'\n",
    "    elif pollutant in ['NOX', 'NO', 'NO2'] or ('NITROGEN' in pollutant_desc and 'OXIDE' in pollutant_desc):\n",
    "        return 'NOx'\n",
    "    elif pollutant == 'NH3' or 'AMMONIA' in pollutant_desc:\n",
    "        return 'NH3'\n",
    "    elif pollutant in ['SO2', 'SO4'] or 'SULFUR' in pollutant_desc:\n",
    "        return 'SOx'\n",
    "    elif 'PM25' in pollutant or 'PM2.5' in pollutant_desc or 'PM2_5' in pollutant:\n",
    "        return 'PM2_5'\n",
    "    elif pollutant == 'CO2':\n",
    "        return 'CO2'\n",
    "    return 'Other'\n",
    "\n",
    "# Apply the conversion and categorization functions\n",
    "print(\"Processing emissions data...\")\n",
    "\n",
    "# Make sure required columns exist\n",
    "required_columns = ['emissions uom', 'total emissions', 'pollutant code', 'pollutant desc', \n",
    "                    'site latitude', 'site longitude']\n",
    "for col in required_columns:\n",
    "    if col not in filtered_nei_df.columns:\n",
    "        print(f\"Warning: Missing required column '{col}' in NEI data\")\n",
    "\n",
    "# Add emissions_tonnes column\n",
    "filtered_nei_df['emissions_tonnes'] = filtered_nei_df.apply(convert_to_tonnes, axis=1)\n",
    "\n",
    "# Add pollutant_category column\n",
    "filtered_nei_df['pollutant_category'] = filtered_nei_df.apply(categorize_pollutant, axis=1)\n",
    "\n",
    "# Ensure latitude and longitude exist before creating geometry\n",
    "filtered_nei_df['geometry'] = filtered_nei_df.apply(\n",
    "    lambda row: Point(row['site longitude'], row['site latitude']) \n",
    "    if pd.notna(row['site longitude']) and pd.notna(row['site latitude']) \n",
    "    else None, axis=1\n",
    ")\n",
    "\n",
    "# Drop rows with invalid geometries\n",
    "filtered_nei_df = filtered_nei_df.dropna(subset=['geometry'])\n",
    "\n",
    "# ================================\n",
    "# Step 5: Aggregate Data by Facility and Create GeoDataFrame\n",
    "# ================================\n",
    "\n",
    "# Group by facility and pollutant category\n",
    "facility_emissions = filtered_nei_df.groupby([\n",
    "    'eis facility id', 'site name', 'state', 'site latitude', 'site longitude', \n",
    "    'primary naics code', 'pollutant_category'\n",
    "])['emissions_tonnes'].sum().reset_index()\n",
    "\n",
    "# Convert to wide format with pollutants as columns\n",
    "facility_wide = facility_emissions.pivot_table(\n",
    "    index=['eis facility id', 'site name', 'state', 'site latitude', 'site longitude', \n",
    "           'primary naics code'],\n",
    "    columns='pollutant_category', \n",
    "    values='emissions_tonnes',\n",
    "    fill_value=0\n",
    ").reset_index()\n",
    "\n",
    "# Ensure all required pollutant columns exist\n",
    "for cat in ['VOC', 'NOx', 'NH3', 'SOx', 'PM2_5', 'CO2']:\n",
    "    if cat not in facility_wide.columns:\n",
    "        facility_wide[cat] = 0\n",
    "\n",
    "# Create geometry column\n",
    "facility_wide['geometry'] = facility_wide.apply(\n",
    "    lambda row: Point(row['site longitude'], row['site latitude']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Add ORISPL and stack parameters from our stored dictionary\n",
    "facility_wide['orispl'] = None\n",
    "facility_wide['height'] = None\n",
    "facility_wide['diam'] = None\n",
    "facility_wide['temp'] = None\n",
    "facility_wide['velocity'] = None\n",
    "\n",
    "# Default stack parameter values (used when real values aren't available)\n",
    "default_stack_height = 50.0  # meters\n",
    "default_stack_diam = 5.0     # meters\n",
    "default_stack_temp = 400.0   # Kelvin\n",
    "default_stack_velocity = 20.0  # m/s\n",
    "\n",
    "# Unit conversion functions\n",
    "def ft_to_m(ft_value):\n",
    "    \"\"\"Convert feet to meters\"\"\"\n",
    "    if pd.isna(ft_value):\n",
    "        return np.nan\n",
    "    return float(ft_value) * 0.3048\n",
    "\n",
    "def f_to_k(f_value):\n",
    "    \"\"\"Convert Fahrenheit to Kelvin\"\"\"\n",
    "    if pd.isna(f_value):\n",
    "        return np.nan\n",
    "    return (float(f_value) - 32) * 5/9 + 273.15\n",
    "\n",
    "# Apply stack parameters from our stored dictionary\n",
    "for idx, row in facility_wide.iterrows():\n",
    "    facility_id = str(row['eis facility id']).strip()\n",
    "    if facility_id in stack_params_by_facility:\n",
    "        params = stack_params_by_facility[facility_id]\n",
    "        facility_wide.at[idx, 'orispl'] = params['orispl']\n",
    "        \n",
    "        # Convert units and apply parameters with fallbacks to defaults\n",
    "        stkhgt = ft_to_m(params['stkhgt'])\n",
    "        facility_wide.at[idx, 'height'] = stkhgt if not pd.isna(stkhgt) else default_stack_height\n",
    "        \n",
    "        stkdiam = ft_to_m(params['stkdiam'])\n",
    "        facility_wide.at[idx, 'diam'] = stkdiam if not pd.isna(stkdiam) else default_stack_diam\n",
    "        \n",
    "        stktemp = f_to_k(params['stktemp'])\n",
    "        facility_wide.at[idx, 'temp'] = stktemp if not pd.isna(stktemp) else default_stack_temp\n",
    "        \n",
    "        stkvel = ft_to_m(params['stkvel'])\n",
    "        facility_wide.at[idx, 'velocity'] = stkvel if not pd.isna(stkvel) else default_stack_velocity\n",
    "    else:\n",
    "        # If we don't have stack parameters for this facility, use defaults\n",
    "        facility_wide.at[idx, 'height'] = default_stack_height\n",
    "        facility_wide.at[idx, 'diam'] = default_stack_diam\n",
    "        facility_wide.at[idx, 'temp'] = default_stack_temp\n",
    "        facility_wide.at[idx, 'velocity'] = default_stack_velocity\n",
    "\n",
    "# Create GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    facility_wide, \n",
    "    geometry='geometry',\n",
    "    crs='epsg:4269'  # Set coordinate reference system\n",
    ")\n",
    "\n",
    "print(f\"Created GeoDataFrame with {len(gdf)} facilities\")\n",
    "\n",
    "# Filter for power plants (EGUs) using NAICS codes\n",
    "egu_naics = ['2211', '221111', '221112', '221113', '221114', '221115', \n",
    "             '221116', '221117', '221118', '221121', '221122']\n",
    "\n",
    "# Create a mask for power plants\n",
    "is_power_plant = gdf['primary naics code'].astype(str).apply(\n",
    "    lambda x: any(x.startswith(prefix) for prefix in egu_naics) if not pd.isna(x) else False\n",
    ")\n",
    "\n",
    "# Apply the mask\n",
    "egu_gdf = gdf[is_power_plant].copy()\n",
    "print(f\"Filtered to {len(egu_gdf)} power plant facilities\")\n",
    "\n",
    "# ================================\n",
    "# Step 7: Save Processed Data\n",
    "# ================================\n",
    "\n",
    "# Save all facilities data\n",
    "all_facilities_output = f\"{output_dir}/processed_all_facilities_emissions.gpkg\"\n",
    "gdf.to_file(all_facilities_output, driver=\"GPKG\")\n",
    "print(f\"Saved all facilities data to {all_facilities_output}\")\n",
    "\n",
    "# Save power plant data\n",
    "egu_output_file = f\"{output_dir}/processed_egu_emissions.gpkg\"\n",
    "egu_gdf.to_file(egu_output_file, driver=\"GPKG\")\n",
    "print(f\"Saved processed power plant data to {egu_output_file}\")\n",
    "\n",
    "# Create a version with just the InMAP required columns plus orispl for reference\n",
    "inmap_columns = [\"orispl\", \"site name\", \"VOC\", \"NOx\", \"NH3\", \"SOx\", \"PM2_5\", \"height\", \"diam\", \"temp\", \"velocity\", \"geometry\"]\n",
    "inmap_egu = egu_gdf[inmap_columns].copy()\n",
    "\n",
    "# Save InMAP-compatible version\n",
    "inmap_output_file = f\"{output_dir}/processed_emissions_for_inmap.gpkg\"\n",
    "inmap_egu.to_file(inmap_output_file, driver=\"GPKG\")\n",
    "print(f\"Saved InMAP-formatted emissions data to {inmap_output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
