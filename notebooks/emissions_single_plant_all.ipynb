{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import zarr\n",
    "import time\n",
    "import s3fs\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# ================================\n",
    "# Helper functions for the SR model\n",
    "# ================================\n",
    "\n",
    "def rect(i, w, s, e, n):\n",
    "    \"\"\"Create rectangle coordinates for grid cell i.\"\"\"\n",
    "    x = [w[i], e[i], e[i], w[i], w[i]]\n",
    "    y = [s[i], s[i], n[i], n[i], s[i]]\n",
    "    return x, y\n",
    "\n",
    "def poly(sr):\n",
    "    \"\"\"Create polygons for all grid cells.\"\"\"\n",
    "    ret = []\n",
    "    w = sr[\"W\"][:]\n",
    "    s = sr[\"S\"][:]\n",
    "    e = sr[\"E\"][:]\n",
    "    n = sr[\"N\"][:]\n",
    "    for i in range(52411):\n",
    "        x, y = rect(i, w, s, e, n)\n",
    "        ret.append(Polygon([[x[0],y[0]],[x[1],y[1]],[x[2],y[2]],\n",
    "                            [x[3],y[3]],[x[4],y[4]]]))\n",
    "    return ret\n",
    "\n",
    "def run_sr(emis, model=\"isrm\", emis_units=\"tons/year\"):\n",
    "    \"\"\"Run the Source-Receptor model for given emissions.\"\"\"\n",
    "    start = time.time()\n",
    "    print(f\"Starting SR model run for {len(emis)} facilities...\")\n",
    "    \n",
    "    # Load spatial receptor grid (SR)\n",
    "    url = 's3://inmap-model/isrm_v1.2.1.zarr/'\n",
    "    fs = s3fs.S3FileSystem(anon=True, client_kwargs={\"region_name\": \"us-east-2\"})\n",
    "    sr = zarr.open(\n",
    "        store=url,\n",
    "        mode=\"r\",\n",
    "        storage_options={\"anon\": True, \"client_kwargs\": {\"region_name\": \"us-east-2\"}}\n",
    "    )   \n",
    "\n",
    "    # Build the grid geometry\n",
    "    p = poly(sr)\n",
    "    print(\"Grid polygons created.\")\n",
    "\n",
    "    # Create grid GeoDataFrame\n",
    "    df = pd.DataFrame({'Location': range(52411)})\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=p, crs=\"+proj=lcc +lat_1=33.000000 +lat_2=45.000000 +lat_0=40.000000 +lon_0=-97.000000 +x_0=0 +y_0=0 +a=6370997.000000 +b=6370997.000000 +to_meter=1\")\n",
    "    \n",
    "    # Ensure emis has CRS set correctly\n",
    "    if emis.crs is None:\n",
    "        print(\"Warning: emis CRS is None. Assigning default CRS (WGS84).\")\n",
    "        emis = emis.set_crs(\"EPSG:4326\")\n",
    "\n",
    "    # Convert emissions to match grid CRS\n",
    "    emis = emis.to_crs(gdf.crs)\n",
    "\n",
    "    # Spatial join (match emissions to grid)\n",
    "    join_right_df = gdf.sjoin(emis, how=\"right\")\n",
    "\n",
    "    # Debugging: Print missing locations\n",
    "    missing_count = join_right_df.Location.isna().sum()\n",
    "    print(f\"Spatial join complete. Missing locations: {missing_count}\")\n",
    "\n",
    "    # Drop NaN locations if any exist\n",
    "    join_right_df = join_right_df.dropna(subset=[\"Location\"])\n",
    "    \n",
    "    index = join_right_df.Location.astype(int).tolist()  # Ensure integer type\n",
    "\n",
    "    # Get unique indices for emissions\n",
    "    ppl = np.unique(index).tolist()\n",
    "\n",
    "    # Create dictionary for mapping locations to index\n",
    "    dictionary = {ppl[i]: i for i in range(len(ppl))}\n",
    "\n",
    "    print(\"Loading SR matrices...\")\n",
    "    # Load Source-Receptor (SR) matrix data\n",
    "    SOA = sr['SOA'].get_orthogonal_selection(([0], ppl, slice(None)))\n",
    "    print(\"- SOA data loaded\")\n",
    "    pNO3 = sr['pNO3'].get_orthogonal_selection(([0], ppl, slice(None)))\n",
    "    print(\"- pNO3 data loaded\")\n",
    "    pNH4 = sr['pNH4'].get_orthogonal_selection(([0], ppl, slice(None)))\n",
    "    print(\"- pNH4 data loaded\")\n",
    "    pSO4 = sr['pSO4'].get_orthogonal_selection(([0], ppl, slice(None)))\n",
    "    print(\"- pSO4 data loaded\")\n",
    "    PM25 = sr['PrimaryPM25'].get_orthogonal_selection(([0], ppl, slice(None)))\n",
    "    print(\"- PrimaryPM25 data loaded\")\n",
    "\n",
    "    # Initialize output data arrays\n",
    "    SOA_data, pNO3_data, pNH4_data, pSO4_data, PM25_data = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "    print(\"Calculating pollution impact...\")\n",
    "    # Calculate pollution data using emissions\n",
    "    for i in range(len(index)):\n",
    "        loc_idx = dictionary[index[i]]  # Get correct index\n",
    "        SOA_data += SOA[0, loc_idx, :] * emis.VOC.iloc[i]\n",
    "        pNO3_data += pNO3[0, loc_idx, :] * emis.NOx.iloc[i]\n",
    "        pNH4_data += pNH4[0, loc_idx, :] * emis.NH3.iloc[i]\n",
    "        pSO4_data += pSO4[0, loc_idx, :] * emis.SOx.iloc[i]\n",
    "        PM25_data += PM25[0, loc_idx, :] * emis.PM2_5.iloc[i]\n",
    "\n",
    "    data = SOA_data + pNO3_data + pNH4_data + pSO4_data + PM25_data\n",
    "\n",
    "    # Apply emission unit conversion factor\n",
    "    fact = 28766.639 if emis_units == \"tons/year\" else 1\n",
    "\n",
    "    print(\"Computing health impacts...\")\n",
    "    # Compute final pollution metrics\n",
    "    TotalPM25 = fact * data\n",
    "    TotalPop = sr['TotalPop'][0:52411]\n",
    "    MortalityRate = sr['MortalityRate'][0:52411]\n",
    "    deathsK = (np.exp(np.log(1.06)/10 * TotalPM25) - 1) * TotalPop * 1.04658 * MortalityRate / 100000 * 1.02523\n",
    "    deathsL = (np.exp(np.log(1.14)/10 * TotalPM25) - 1) * TotalPop * 1.04658 * MortalityRate / 100000 * 1.02523\n",
    "\n",
    "    # Create output GeoDataFrame\n",
    "    ret = gpd.GeoDataFrame(pd.DataFrame({\n",
    "        'SOA': fact * SOA_data,\n",
    "        'pNO3': fact * pNO3_data,\n",
    "        'pNH4': fact * pNH4_data,\n",
    "        'pSO4': fact * pSO4_data,\n",
    "        'PrimaryPM25': fact * PM25_data,\n",
    "        'TotalPM25': TotalPM25,\n",
    "        'deathsK': deathsK,\n",
    "        'deathsL': deathsL\n",
    "    }), geometry=p[:52411], crs=gdf.crs)\n",
    "\n",
    "    print(f\"SR model run complete in {time.time() - start:.0f} seconds\")\n",
    "    return ret\n",
    "\n",
    "# ================================\n",
    "# Functions for single plant analysis\n",
    "# ================================\n",
    "\n",
    "def load_data(nei_file_path, counties_shapefile_path):\n",
    "    \"\"\"Load NEI data and county boundaries.\"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    \n",
    "    # Load NEI facility data\n",
    "    try:\n",
    "        df = pd.read_csv(nei_file_path, sep=',', low_memory=False)\n",
    "        print(f\"NEI data loaded with shape: {df.shape}\")\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            df = pd.read_csv(nei_file_path, low_memory=False)\n",
    "            print(f\"NEI data loaded with shape: {df.shape}\")\n",
    "        except Exception as e2:\n",
    "            print(f\"Error loading NEI data: {e2}\")\n",
    "            return None, None\n",
    "    \n",
    "    # Process NEI data as in your original script\n",
    "    # Convert emissions to metric tonnes\n",
    "    def convert_to_tonnes(row):\n",
    "        if row['emissions uom'] == 'LB':\n",
    "            return float(row['total emissions']) * 0.000453592  # Convert pounds to metric tonnes\n",
    "        elif row['emissions uom'] == 'TON':\n",
    "            return float(row['total emissions']) * 0.90718474  # Convert short tons to metric tonnes\n",
    "        return float(row['total emissions'])  # Already in metric tonnes\n",
    "\n",
    "    df['emissions_tonnes'] = df.apply(convert_to_tonnes, axis=1)\n",
    "\n",
    "    # Categorize pollutants\n",
    "    def categorize_pollutant(row):\n",
    "        pollutant = str(row['pollutant code']).upper()\n",
    "        pollutant_desc = str(row['pollutant desc']).upper()\n",
    "\n",
    "        if pollutant == 'VOC' or 'VOLATILE ORGANIC' in pollutant_desc:\n",
    "            return 'VOC'\n",
    "        elif pollutant in ['NOX', 'NO', 'NO2'] or ('NITROGEN' in pollutant_desc and 'OXIDE' in pollutant_desc):\n",
    "            return 'NOx'\n",
    "        elif pollutant == 'NH3' or 'AMMONIA' in pollutant_desc:\n",
    "            return 'NH3'\n",
    "        elif pollutant in ['SO2', 'SO4'] or 'SULFUR' in pollutant_desc:\n",
    "            return 'SOx'\n",
    "        elif 'PM25' in pollutant or 'PM2.5' in pollutant_desc or 'PM2_5' in pollutant:\n",
    "            return 'PM2_5'\n",
    "        return 'Other'\n",
    "\n",
    "    df['pollutant_category'] = df.apply(categorize_pollutant, axis=1)\n",
    "\n",
    "    # Filter out Alaska & Hawaii\n",
    "    df = df[~df['state'].isin(['AK', 'HI'])]\n",
    "\n",
    "    # Aggregate data by facility\n",
    "    facility_emissions = df.groupby([\n",
    "        'eis facility id', 'site name', 'state', 'site latitude', 'site longitude', \n",
    "        'primary naics code', 'primary naics description', 'pollutant_category'\n",
    "    ])['emissions_tonnes'].sum().reset_index()\n",
    "\n",
    "    # Convert to wide format with pollutants as columns\n",
    "    facility_wide = facility_emissions.pivot_table(\n",
    "        index=['eis facility id', 'site name', 'state', 'site latitude', 'site longitude', \n",
    "               'primary naics code', 'primary naics description'],\n",
    "        columns='pollutant_category', \n",
    "        values='emissions_tonnes',\n",
    "        fill_value=0\n",
    "    ).reset_index()\n",
    "\n",
    "    # Ensure all required pollutant columns exist\n",
    "    for cat in ['VOC', 'NOx', 'NH3', 'SOx', 'PM2_5']:\n",
    "        if cat not in facility_wide.columns:\n",
    "            facility_wide[cat] = 0\n",
    "\n",
    "    # Create GeoDataFrame\n",
    "    facility_wide['geometry'] = facility_wide.apply(lambda row: Point(row['site longitude'], row['site latitude']), axis=1)\n",
    "    gdf = gpd.GeoDataFrame(facility_wide, geometry='geometry', crs='epsg:4269')\n",
    "\n",
    "    # Filter for power plants (EGUs) using NAICS codes\n",
    "    egu_naics = ['2211', '221111', '221112', '221113', '221114', '221115', '221116', '221117', '221118']\n",
    "    egu_gdf = gdf[gdf['primary naics code'].astype(str).str.startswith(tuple(egu_naics))]\n",
    "    print(f\"Found {len(egu_gdf)} power plant facilities\")\n",
    "\n",
    "    # Load county boundaries\n",
    "    try:\n",
    "        us_counties = gpd.read_file(counties_shapefile_path)\n",
    "        print(f\"County boundaries loaded with {len(us_counties)} counties\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading county boundaries: {e}\")\n",
    "        return egu_gdf, None\n",
    "    \n",
    "    return egu_gdf, us_counties\n",
    "\n",
    "def list_facilities(egu_gdf, n=20):\n",
    "    \"\"\"Display a list of facilities for selection.\"\"\"\n",
    "    sample = egu_gdf[['eis facility id', 'site name', 'state', 'NOx', 'SOx', 'PM2_5']].drop_duplicates()\n",
    "    sample = sample.sort_values('NOx', ascending=False).head(n)\n",
    "    \n",
    "    # Format the emissions columns\n",
    "    for col in ['NOx', 'SOx', 'PM2_5']:\n",
    "        sample[col] = sample[col].map(lambda x: f\"{x:.1f}\")\n",
    "    \n",
    "    sample.columns = ['Facility ID', 'Facility Name', 'State', 'NOx (tonnes)', 'SOx (tonnes)', 'PM2.5 (tonnes)']\n",
    "    return sample\n",
    "\n",
    "def analyze_single_plant(egu_gdf, facility_id):\n",
    "    \"\"\"Analyze health impacts from a single facility.\"\"\"\n",
    "    # Filter for just one facility\n",
    "    single_plant = egu_gdf[egu_gdf['eis facility id'] == facility_id].copy()\n",
    "    \n",
    "    if len(single_plant) == 0:\n",
    "        print(f\"Facility ID {facility_id} not found.\")\n",
    "        return None\n",
    "        \n",
    "    print(f\"Analyzing facility: {single_plant['site name'].iloc[0]}\")\n",
    "    print(f\"Location: {single_plant['state'].iloc[0]}\")\n",
    "    print(f\"Emissions (tonnes): NOx={single_plant['NOx'].iloc[0]:.2f}, SOx={single_plant['SOx'].iloc[0]:.2f}, PM2.5={single_plant['PM2_5'].iloc[0]:.2f}\")\n",
    "    \n",
    "    # Run the SR model on just this facility\n",
    "    results = run_sr(single_plant, model=\"isrm\", emis_units=\"tons/year\")\n",
    "    \n",
    "    return results, single_plant\n",
    "\n",
    "def calculate_plant_county_impacts(results, us_counties):\n",
    "    \"\"\"Calculate county-level health impacts for a single plant.\"\"\"\n",
    "    # Convert counties to match results CRS if needed\n",
    "    us_counties = us_counties.to_crs(results.crs)\n",
    "    \n",
    "    # Exclude Alaska, Hawaii, and Puerto Rico using STATEFP codes\n",
    "    us_counties = us_counties[~us_counties['STATEFP'].isin([\"02\", \"15\", \"72\"])]\n",
    "    \n",
    "    # Perform spatial join to assign each grid cell to a county\n",
    "    results_county = results.sjoin(us_counties, how=\"left\", predicate=\"intersects\")\n",
    "    \n",
    "    # Aggregate health impacts by county and state\n",
    "    county_summary = results_county.groupby([\"STATEFP\", \"NAME\"]).agg({\n",
    "        \"TotalPM25\": \"mean\",  # Average PM2.5 concentration\n",
    "        \"deathsK\": \"sum\",     # Total premature deaths\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Calculate health damages using VSL\n",
    "    VSL = 13.2e6  # Value of a Statistical Life in dollars\n",
    "    county_summary['HealthDamages'] = county_summary['deathsK'] * VSL\n",
    "    \n",
    "    # Add state names\n",
    "    state_fips = {\n",
    "        '01': 'Alabama', '04': 'Arizona', '05': 'Arkansas', '06': 'California', \n",
    "        '08': 'Colorado', '09': 'Connecticut', '10': 'Delaware', '11': 'District of Columbia',\n",
    "        '12': 'Florida', '13': 'Georgia', '16': 'Idaho', '17': 'Illinois', '18': 'Indiana',\n",
    "        '19': 'Iowa', '20': 'Kansas', '21': 'Kentucky', '22': 'Louisiana', '23': 'Maine',\n",
    "        '24': 'Maryland', '25': 'Massachusetts', '26': 'Michigan', '27': 'Minnesota',\n",
    "        '28': 'Mississippi', '29': 'Missouri', '30': 'Montana', '31': 'Nebraska',\n",
    "        '32': 'Nevada', '33': 'New Hampshire', '34': 'New Jersey', '35': 'New Mexico',\n",
    "        '36': 'New York', '37': 'North Carolina', '38': 'North Dakota', '39': 'Ohio',\n",
    "        '40': 'Oklahoma', '41': 'Oregon', '42': 'Pennsylvania', '44': 'Rhode Island',\n",
    "        '45': 'South Carolina', '46': 'South Dakota', '47': 'Tennessee', '48': 'Texas',\n",
    "        '49': 'Utah', '50': 'Vermont', '51': 'Virginia', '53': 'Washington',\n",
    "        '54': 'West Virginia', '55': 'Wisconsin', '56': 'Wyoming'\n",
    "    }\n",
    "    \n",
    "    county_summary['State'] = county_summary['STATEFP'].map(state_fips)\n",
    "    \n",
    "    # Format the county name with state\n",
    "    county_summary['County'] = county_summary['NAME'] + ', ' + county_summary['State']\n",
    "    \n",
    "    return county_summary\n",
    "\n",
    "def plot_single_plant_impacts(county_summary, us_counties, facility_name, facility_state):\n",
    "    \"\"\"Create a map visualization of single plant impacts.\"\"\"\n",
    "    # Merge summary with county shapefile\n",
    "    us_counties = us_counties.to_crs('epsg:4269')  # Ensure consistent CRS\n",
    "    counties_with_impacts = us_counties.merge(\n",
    "        county_summary[['NAME', 'STATEFP', 'HealthDamages']], \n",
    "        on=['NAME', 'STATEFP'], \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Fill NaN values with 0\n",
    "    counties_with_impacts['HealthDamages'] = counties_with_impacts['HealthDamages'].fillna(0)\n",
    "    \n",
    "    # Create bins and colors for the map\n",
    "    # Adjust bins based on the actual damage values\n",
    "    max_damage = counties_with_impacts['HealthDamages'].max()\n",
    "    \n",
    "    if max_damage < 1e6:  # Less than $1M max\n",
    "        bins = [0, 1e3, 5e3, 1e4, 5e4, 1e5, 2e5, 5e5, max_damage * 0.9, float(\"inf\")]\n",
    "        legend_labels = [\n",
    "            \"$0 - $1K\", \"$1K - $5K\", \"$5K - $10K\", \"$10K - $50K\", \n",
    "            \"$50K - $100K\", \"$100K - $200K\", \"$200K - $500K\", \n",
    "            f\"$500K - ${int(max_damage * 0.9/1000)}K\", f\"${int(max_damage * 0.9/1000)}K+\"\n",
    "        ]\n",
    "    else:  # More than $1M max\n",
    "        bins = [0, 1e4, 5e4, 1e5, 5e5, 1e6, 5e6, 1e7, max_damage * 0.9, float(\"inf\")]\n",
    "        legend_labels = [\n",
    "            \"$0 - $10K\", \"$10K - $50K\", \"$50K - $100K\", \"$100K - $500K\", \n",
    "            \"$500K - $1M\", \"$1M - $5M\", \"$5M - $10M\", \n",
    "            f\"$10M - ${int(max_damage * 0.9/1000000)}M\", f\"${int(max_damage * 0.9/1000000)}M+\"\n",
    "        ]\n",
    "    \n",
    "    colors = ['#ffedea', '#ffcec5', '#ffad9f', '#ff7f66', '#ff4d33', \n",
    "              '#ff1a00', '#cc1600', '#990f00', '#660a00', '#400600']\n",
    "    \n",
    "    cmap = mcolors.ListedColormap(colors)\n",
    "    counties_with_impacts['HealthDamages_Binned'] = pd.cut(\n",
    "        counties_with_impacts['HealthDamages'], bins=bins, labels=False, include_lowest=True)\n",
    "    \n",
    "    # Create the map\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    counties_with_impacts.plot(column='HealthDamages_Binned', cmap=cmap, \n",
    "                               linewidth=0.3, edgecolor=\"black\", ax=ax, legend=False)\n",
    "    \n",
    "    # Highlight the state where the plant is located\n",
    "    plant_state = counties_with_impacts[counties_with_impacts['STUSPS'] == facility_state]\n",
    "    plant_state.boundary.plot(ax=ax, color='blue', linewidth=1.0)\n",
    "    \n",
    "    # Add title and formatting\n",
    "    ax.set_title(f\"Health Damages from {facility_name} ({facility_state})\", fontsize=14)\n",
    "    ax.axis('off')\n",
    "    ax.set_aspect(1.3)\n",
    "    \n",
    "    # Set limits to properly zoom into the contiguous U.S.\n",
    "    ax.set_xlim(-130, -60)  # Longitude limits\n",
    "    ax.set_ylim(20, 55)     # Latitude limits\n",
    "    \n",
    "    # Create legend\n",
    "    legend_patches = [mpatches.Patch(color=colors[i], label=legend_labels[i]) \n",
    "                     for i in range(len(legend_labels))]\n",
    "    \n",
    "    ax.legend(handles=legend_patches, title=\"Health Damages ($)\", loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig, counties_with_impacts\n",
    "\n",
    "def calculate_national_totals(results):\n",
    "    \"\"\"Calculate national total impacts.\"\"\"\n",
    "    total_pm25 = results['TotalPM25'].sum()\n",
    "    total_deaths = results['deathsK'].sum()\n",
    "    total_damages = total_deaths * 13.2e6  # VSL in dollars\n",
    "    \n",
    "    return {\n",
    "        'total_pm25': total_pm25,\n",
    "        'total_deaths': total_deaths,\n",
    "        'total_damages': total_damages\n",
    "    }\n",
    "\n",
    "def compare_multiple_plants(egu_gdf, us_counties, facility_ids):\n",
    "    \"\"\"Compare impacts from multiple facilities.\"\"\"\n",
    "    results_list = []\n",
    "    \n",
    "    for facility_id in facility_ids:\n",
    "        single_plant = egu_gdf[egu_gdf['eis facility id'] == facility_id].copy()\n",
    "        \n",
    "        if len(single_plant) == 0:\n",
    "            print(f\"Facility ID {facility_id} not found.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Analyzing facility: {single_plant['site name'].iloc[0]}\")\n",
    "        \n",
    "        # Run the SR model for this facility\n",
    "        sr_results, _ = analyze_single_plant(egu_gdf, facility_id)\n",
    "        \n",
    "        # Calculate county impacts\n",
    "        county_impacts = calculate_plant_county_impacts(sr_results, us_counties)\n",
    "        \n",
    "        # Calculate national totals\n",
    "        national_totals = calculate_national_totals(sr_results)\n",
    "        \n",
    "        # Compile results\n",
    "        facility_name = single_plant['site name'].iloc[0]\n",
    "        facility_state = single_plant['state'].iloc[0]\n",
    "        \n",
    "        results_list.append({\n",
    "            'facility_id': facility_id,\n",
    "            'facility_name': facility_name,\n",
    "            'state': facility_state,\n",
    "            'nox_emissions': single_plant['NOx'].iloc[0],\n",
    "            'sox_emissions': single_plant['SOx'].iloc[0],\n",
    "            'pm25_emissions': single_plant['PM2_5'].iloc[0],\n",
    "            'total_deaths': national_totals['total_deaths'],\n",
    "            'total_damages': national_totals['total_damages'],\n",
    "            'county_impacts': county_impacts\n",
    "        })\n",
    "    \n",
    "    return results_list\n",
    "\n",
    "# ================================\n",
    "# Main function to run the analysis\n",
    "# ================================\n",
    "\n",
    "def main(nei_file_path, counties_shapefile_path, analysis_type='single', facility_ids=None):\n",
    "    \"\"\"Main function to run the single plant analysis.\"\"\"\n",
    "    # Load data\n",
    "    egu_gdf, us_counties = load_data(nei_file_path, counties_shapefile_path)\n",
    "    \n",
    "    if egu_gdf is None:\n",
    "        print(\"Error loading data. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Display sample facilities for selection\n",
    "    print(\"\\nTop facilities by NOx emissions:\")\n",
    "    sample_facilities = list_facilities(egu_gdf)\n",
    "    print(sample_facilities)\n",
    "    \n",
    "    if analysis_type == 'single' and facility_ids is None:\n",
    "        # Select first facility by default or let user specify\n",
    "        facility_id = sample_facilities['Facility ID'].iloc[0]\n",
    "        print(f\"\\nAnalyzing facility ID: {facility_id}\")\n",
    "        \n",
    "        # Run analysis for single plant\n",
    "        results, plant_info = analyze_single_plant(egu_gdf, facility_id)\n",
    "        \n",
    "        if results is not None:\n",
    "            # Calculate county impacts\n",
    "            county_impacts = calculate_plant_county_impacts(results, us_counties)\n",
    "            \n",
    "            # Display top impacted counties\n",
    "            print(\"\\nTop 10 counties with highest health damages:\")\n",
    "            top_counties = county_impacts.sort_values('HealthDamages', ascending=False).head(10)\n",
    "            top_counties['HealthDamages_Millions'] = top_counties['HealthDamages'] / 1e6\n",
    "            print(top_counties[['County', 'HealthDamages_Millions']].rename(\n",
    "                columns={'HealthDamages_Millions': 'Health Damages ($ millions)'}))\n",
    "            \n",
    "            # Calculate national totals\n",
    "            national_totals = calculate_national_totals(results)\n",
    "            print(\"\\nNational Totals:\")\n",
    "            print(f\"Total premature deaths: {national_totals['total_deaths']:.2f}\")\n",
    "            print(f\"Total health damages: ${national_totals['total_damages']/1e6:.2f} million\")\n",
    "            \n",
    "            # Create visualization\n",
    "            facility_name = plant_info['site name'].iloc[0]\n",
    "            facility_state = plant_info['state'].iloc[0]\n",
    "            fig, counties_with_data = plot_single_plant_impacts(county_impacts, us_counties, facility_name, facility_state)\n",
    "            \n",
    "            # Save results\n",
    "            output_dir = \"./outputs\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "            fig.savefig(f\"{output_dir}/health_impacts_{facility_id}.png\", dpi=300, bbox_inches='tight')\n",
    "            county_impacts.to_csv(f\"{output_dir}/county_impacts_{facility_id}.csv\", index=False)\n",
    "            \n",
    "            print(f\"\\nResults saved to {output_dir}/\")\n",
    "            \n",
    "            # Show plot\n",
    "            plt.show()\n",
    "    \n",
    "    elif analysis_type == 'compare' or facility_ids is not None:\n",
    "        # Ensure we have facility IDs to compare\n",
    "        if facility_ids is None:\n",
    "            # Use top 5 facilities by default\n",
    "            facility_ids = sample_facilities['Facility ID'].head(5).tolist()\n",
    "        \n",
    "        print(f\"\\nComparing {len(facility_ids)} facilities...\")\n",
    "        \n",
    "        # Run comparison analysis\n",
    "        comparison_results = compare_multiple_plants(egu_gdf, us_counties, facility_ids)\n",
    "        \n",
    "        # Display comparison summary\n",
    "        print(\"\\nComparison Summary:\")\n",
    "        summary_data = []\n",
    "        for result in comparison_results:\n",
    "            summary_data.append({\n",
    "                'Facility Name': result['facility_name'],\n",
    "                'State': result['state'],\n",
    "                'NOx (tons)': result['nox_emissions'],\n",
    "                'SOx (tons)': result['sox_emissions'],\n",
    "                'PM2.5 (tons)': result['pm25_emissions'],\n",
    "                'Premature Deaths': result['total_deaths'],\n",
    "                'Health Damages ($ millions)': result['total_damages'] / 1e6\n",
    "            })\n",
    "        \n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        print(summary_df.sort_values('Health Damages ($ millions)', ascending=False))\n",
    "        \n",
    "        # Save comparison results\n",
    "        output_dir = \"./outputs\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        summary_df.to_csv(f\"{output_dir}/facility_comparison.csv\", index=False)\n",
    "        print(f\"\\nComparison results saved to {output_dir}/facility_comparison.csv\")\n",
    "    \n",
    "    return egu_gdf, us_counties\n",
    "\n",
    "# ================================\n",
    "# Example usage\n",
    "# ================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Paths to your data files\n",
    "    nei_file_path = \"../data/raw/2021_NEI_Facility_summary.csv\"\n",
    "    counties_shapefile_path = \"../data/raw/cb_2018_us_county_500k/cb_2018_us_county_500k.shp\"\n",
    "    \n",
    "    # Run single facility analysis\n",
    "    egu_gdf, us_counties = main(nei_file_path, counties_shapefile_path)\n",
    "    \n",
    "    # Alternative: Compare multiple facilities\n",
    "    # facility_ids = [12345, 67890, 13579]  # Replace with actual facility IDs\n",
    "    # main(nei_file_path, counties_shapefile_path, analysis_type='compare', facility_ids=facility_ids)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
